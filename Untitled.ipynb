#I decided to first use a news API to get a notion of what key words return in news article in order to optimize my search for relevant keywords.
#The following code shows the retrieval of the word cloud included in the assignment, which resulted from querying the word business in news articles. 

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import requests  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret ='ac3440d110394aaa82784528501f8b67'\n",
    "url = 'https://newsapi.org/v2/everything?'\n",
    "parameters = {\n",
    "    'q': 'business', # query phrase\n",
    "    'pageSize': 20,  # maximum is 100\n",
    "    'apiKey': secret # your own API key\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "\n",
    "response_json = response.json()\n",
    "#pprint.pprint(response_json)\n",
    "\n",
    "#for i in response_json['articles']:\n",
    "#    print(i['title'])\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an empty string\n",
    "text_combined = ''\n",
    "# Loop through all the headlines and add them to 'text_combined' \n",
    "for i in response_json['articles']:\n",
    "    text_combined += i['title'] + ' ' # add a space after every headline, so the first and last words are not glued together\n",
    "wordcloud = WordCloud(max_font_size=40).generate(text_combined)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('articles1.csv')\n",
    "df2 = pd.read_csv('articles2.csv')\n",
    "df3 = pd.read_csv('articles3.csv')\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "df = pd.concat([df,df2], sort=False)\n",
    "df = pd.concat([df,df3], sort=False)\n",
    "df['date'] =pd.to_datetime(df.date)\n",
    "df = df.sort_values('date') # This now sorts in \n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "\n",
    "del df['Unnamed: 0']\n",
    "del df['index']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy = df[df['content'].astype(str).str.contains('economy', case=False)]\n",
    "rates = df[df['content'].astype(str).str.contains('interest rates', case=False)]\n",
    "unemployment = df[df['content'].astype(str).str.contains('unemployment|employment', case=False)]\n",
    "inflation = df[df['content'].astype(str).str.contains('inflation', case=False)]\n",
    "policy = df[df['content'].astype(str).str.contains('fiscal policy|monetary policy', case=False)]\n",
    "trade = df[df['content'].astype(str).str.contains('trade', case=False)]\n",
    "wages = df[df['content'].astype(str).str.contains('wages|wage|salaries|salary', case=False)]\n",
    "savings = df[df['content'].astype(str).str.contains('savings', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy = economy.set_index(['date'])\n",
    "rates= rates.set_index(['date'])\n",
    "unemployment = unemployment.set_index(['date'])\n",
    "inflation = inflation.set_index(['date'])\n",
    "policy = policy.set_index(['date'])\n",
    "trade = trade.set_index(['date'])\n",
    "wages = wages.set_index(['date'])\n",
    "savings = savings.set_index(['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word, Blobber\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.taggers import NLTKTagger\n",
    "\n",
    "\n",
    "economy[['polarity', 'subjectivity']] = economy['content'].apply(lambda content: pd.Series(TextBlob(content).sentiment))\n",
    "rates[['polarity', 'subjectivity']] = rates['content'].apply(lambda content: pd.Series(TextBlob(content).sentiment))\n",
    "unemployment[['polarity', 'subjectivity']] = unemployment['content'].apply(lambda content: pd.Series(TextBlob(content).sentiment))\n",
    "inflation[['polarity', 'subjectivity']] = inflation['content'].apply(lambda content: pd.Series(TextBlob(content).sentiment))\n",
    "trade[['polarity', 'subjectivity']] = trade['content'].apply(lambda content: pd.Series(TextBlob(content).sentiment))\n",
    "wages[['polarity', 'subjectivity']] = wages['content'].apply(lambda content: pd.Series(TextBlob(content).sentiment))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word, Blobber\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.taggers import NLTKTagger\n",
    "\n",
    "\n",
    "economy[['polarity_title', 'subjectivity_title']] = economy['title'].apply(lambda title: pd.Series(TextBlob(title).sentiment))\n",
    "rates[['polarity_title', 'subjectivity_title']] = rates['title'].apply(lambda title: pd.Series(TextBlob(title).sentiment))\n",
    "unemployment[['polarity_title', 'subjectivity_title']] = unemployment['title'].apply(lambda title: pd.Series(TextBlob(title).sentiment))\n",
    "inflation[['polarity_title', 'subjectivity_title']] = inflation['title'].apply(lambda title: pd.Series(TextBlob(title).sentiment))\n",
    "trade[['polarity_title', 'subjectivity_title']] = trade['title'].apply(lambda title: pd.Series(TextBlob(title).sentiment))\n",
    "wages[['polarity_title', 'subjectivity_title']] = wages['title'].apply(lambda title: pd.Series(TextBlob(title).sentiment))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_confidence = pd.read_csv(\"consumerconfus.csv\")\n",
    "consumer_confidence['TIME'] = pd.to_datetime(consumer_confidence.TIME)\n",
    "del consumer_confidence['MEASURE']\n",
    "del consumer_confidence['Flag Codes']\n",
    "del consumer_confidence['SUBJECT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_confidence['LOCATION'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA = consumer_confidence[consumer_confidence['LOCATION']=='USA']\n",
    "change_2015 = USA[USA['TIME'].dt.year == 2015]\n",
    "change_2015 = change_2015.groupby(change_2015.TIME.dt.month).mean()\n",
    "\n",
    "change_2016 = USA[USA['TIME'].dt.year == 2016]\n",
    "change_2016 = change_2016.groupby(change_2016.TIME.dt.month).mean()\n",
    "\n",
    "change_2017 = USA[USA['TIME'].dt.year == 2017]\n",
    "change_2017 = change_2017.groupby(change_2017.TIME.dt.month).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA19 = consumer_confidence[consumer_confidence['LOCATION']=='EA19']\n",
    "change_2015_EU = EA19[EA19['TIME'].dt.year == 2015]\n",
    "change_2015_EU = change_2015_EU.groupby(change_2015_EU.TIME.dt.month).mean()\n",
    "\n",
    "change_2016_EU = EA19[EA19['TIME'].dt.year == 2016]\n",
    "change_2016_EU = change_2016_EU.groupby(change_2016_EU.TIME.dt.month).mean()\n",
    "\n",
    "change_2017_EU = EA19[EA19['TIME'].dt.year == 2017]\n",
    "change_2017_EU = change_2017_EU.groupby(change_2017_EU.TIME.dt.month).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change_2015.pct_change().max()*100)\n",
    "print(change_2016.pct_change().max()*100)\n",
    "print(change_2017.pct_change().max()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change_2015_EU.pct_change().max()*100)\n",
    "print(change_2016_EU.pct_change().max()*100)\n",
    "print(change_2017_EU.pct_change().max()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy['key'] = economy['year'].astype(str) + economy['month'].astype(str)\n",
    "rates['key'] = rates['year'].astype(str) + rates['month'].astype(str)\n",
    "unemployment['key'] = unemployment['year'].astype(str) + unemployment['month'].astype(str)\n",
    "wages['key'] = wages['year'].astype(str) + wages['month'].astype(str)\n",
    "inflation['key'] = inflation['year'].astype(str) + inflation['month'].astype(str)\n",
    "trade['key'] = trade['year'].astype(str) + trade['month'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy = economy['2014-1-1':'2017-4-1']\n",
    "rates = rates['2014-1-1':'2017-4-1']\n",
    "unemployment = unemployment['2014-1-1':'2017-4-1']\n",
    "wages = wages['2014-1-1':'2017-4-1']\n",
    "inflation = inflation['2014-1-1':'2017-4-1']\n",
    "trade = trade['2014-1-1':'2017-4-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = economy.groupby(['key']).polarity.mean()\n",
    "s = df_mean.to_dict()\n",
    "economy['polarity_grouped'] = economy.key.replace(s)\n",
    "\n",
    "df_mean = rates.groupby(['key']).polarity.mean()\n",
    "s = df_mean.to_dict()\n",
    "rates['polarity_grouped'] = rates.key.replace(s)\n",
    "\n",
    "df_mean = unemployment.groupby(['key']).polarity.mean()\n",
    "s = df_mean.to_dict()\n",
    "unemployment['polarity_grouped'] = unemployment.key.replace(s)\n",
    "\n",
    "df_mean = wages.groupby(['key']).polarity.mean()\n",
    "s = df_mean.to_dict()\n",
    "wages['polarity_grouped'] = wages.key.replace(s)\n",
    "\n",
    "df_mean = inflation.groupby(['key']).polarity.mean()\n",
    "s = df_mean.to_dict()\n",
    "inflation['polarity_grouped'] = inflation.key.replace(s)\n",
    "\n",
    "df_mean = trade.groupby(['key']).polarity.mean()\n",
    "s = df_mean.to_dict()\n",
    "trade['polarity_grouped'] = trade.key.replace(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = economy.groupby(['key']).polarity_title.mean()\n",
    "s = df_mean.to_dict()\n",
    "economy['polarity_title_grouped'] = economy.key.replace(s)\n",
    "\n",
    "df_mean = rates.groupby(['key']).polarity_title.mean()\n",
    "s = df_mean.to_dict()\n",
    "rates['polarity_title_grouped'] = rates.key.replace(s)\n",
    "\n",
    "df_mean = unemployment.groupby(['key']).polarity_title.mean()\n",
    "s = df_mean.to_dict()\n",
    "unemployment['polarity_title_grouped'] = unemployment.key.replace(s)\n",
    "\n",
    "df_mean = wages.groupby(['key']).polarity_title.mean()\n",
    "s = df_mean.to_dict()\n",
    "wages['polarity_title_grouped'] = wages.key.replace(s)\n",
    "\n",
    "df_mean = inflation.groupby(['key']).polarity_title.mean()\n",
    "s = df_mean.to_dict()\n",
    "inflation['polarity_title_grouped'] = inflation.key.replace(s)\n",
    "\n",
    "df_mean = trade.groupby(['key']).polarity_title.mean()\n",
    "s = df_mean.to_dict()\n",
    "trade['polarity_title_grouped'] = trade.key.replace(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy['TIME'] = economy.index\n",
    "rates['TIME'] = rates.index\n",
    "unemployment['TIME'] = unemployment.index\n",
    "wages['TIME'] = wages.index\n",
    "inflation['TIME'] = inflation.index\n",
    "trade['TIME'] = trade.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy['TIME'] = pd.to_datetime(economy['TIME'],format='%d%m%Y')\n",
    "economy.loc[economy[\"TIME\"] != 1,'First_Day'] = economy['TIME'] + pd.offsets.MonthBegin(1)\n",
    "economy['year'], economy['month'], economy['day'] = economy['First_Day'].dt.year, economy['First_Day'].dt.month,economy['First_Day'].dt.day\n",
    "\n",
    "rates['TIME'] = pd.to_datetime(rates['TIME'],format='%d%m%Y')\n",
    "rates.loc[rates[\"TIME\"] != 1,'First_Day'] = rates['TIME'] + pd.offsets.MonthBegin(1)\n",
    "rates['year'], rates['month'], rates['day'] = rates['First_Day'].dt.year, rates['First_Day'].dt.month,rates['First_Day'].dt.day\n",
    "\n",
    "unemployment['TIME'] = pd.to_datetime(unemployment['TIME'],format='%d%m%Y')\n",
    "unemployment.loc[unemployment[\"TIME\"] != 1,'First_Day'] = unemployment['TIME'] + pd.offsets.MonthBegin(1)\n",
    "unemployment['year'], unemployment['month'], unemployment['day'] = unemployment['First_Day'].dt.year, unemployment['First_Day'].dt.month,unemployment['First_Day'].dt.day\n",
    "\n",
    "wages['TIME'] = pd.to_datetime(wages['TIME'],format='%d%m%Y')\n",
    "wages.loc[wages[\"TIME\"] != 1,'First_Day'] = wages['TIME'] + pd.offsets.MonthBegin(1)\n",
    "wages['year'], wages['month'], wages['day'] = wages['First_Day'].dt.year, wages['First_Day'].dt.month,wages['First_Day'].dt.day\n",
    "\n",
    "inflation['TIME'] = pd.to_datetime(inflation['TIME'],format='%d%m%Y')\n",
    "inflation.loc[inflation[\"TIME\"] != 1,'First_Day'] = inflation['TIME'] + pd.offsets.MonthBegin(1)\n",
    "inflation['year'], inflation['month'], inflation['day'] = inflation['First_Day'].dt.year, inflation['First_Day'].dt.month,inflation['First_Day'].dt.day\n",
    "\n",
    "trade['TIME'] = pd.to_datetime(trade['TIME'],format='%d%m%Y')\n",
    "trade.loc[trade[\"TIME\"] != 1,'First_Day'] = trade['TIME'] + pd.offsets.MonthBegin(1)\n",
    "trade['year'], trade['month'], trade['day'] = trade['First_Day'].dt.year, trade['First_Day'].dt.month,trade['First_Day'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA['year'], USA['month'], USA['day'] = USA['TIME'].dt.year, USA['TIME'].dt.month,USA['TIME'].dt.day\n",
    "economy['key2'] = economy['year'].astype(str) + economy['month'].astype(str) + economy['day'].astype(str)\n",
    "USA['key'] = USA['year'].astype(str) + USA['month'].astype(str) + USA['day'].astype(str)\n",
    "mapping = USA.set_index('key').to_dict()['Value']\n",
    "\n",
    "economy['Value'] = economy['key2'].replace(mapping)\n",
    "\n",
    "\n",
    "USA['year'], USA['month'], USA['day'] = USA['TIME'].dt.year, USA['TIME'].dt.month,USA['TIME'].dt.day\n",
    "rates['key2'] = rates['year'].astype(str) + rates['month'].astype(str) + rates['day'].astype(str)\n",
    "USA['key'] = USA['year'].astype(str) + USA['month'].astype(str) + USA['day'].astype(str)\n",
    "mapping = USA.set_index('key').to_dict()['Value']\n",
    "\n",
    "rates['Value'] = rates['key2'].replace(mapping)\n",
    "\n",
    "\n",
    "USA['year'], USA['month'], USA['day'] = USA['TIME'].dt.year, USA['TIME'].dt.month,USA['TIME'].dt.day\n",
    "unemployment['key2'] = unemployment['year'].astype(str) + unemployment['month'].astype(str) + unemployment['day'].astype(str)\n",
    "USA['key'] = USA['year'].astype(str) + USA['month'].astype(str) + USA['day'].astype(str)\n",
    "mapping = USA.set_index('key').to_dict()['Value']\n",
    "\n",
    "unemployment['Value'] = unemployment['key2'].replace(mapping)\n",
    "\n",
    "\n",
    "USA['year'], USA['month'], USA['day'] = USA['TIME'].dt.year, USA['TIME'].dt.month,USA['TIME'].dt.day\n",
    "wages['key2'] = wages['year'].astype(str) + wages['month'].astype(str) + wages['day'].astype(str)\n",
    "USA['key'] = USA['year'].astype(str) + USA['month'].astype(str) + USA['day'].astype(str)\n",
    "mapping = USA.set_index('key').to_dict()['Value']\n",
    "\n",
    "wages['Value'] = wages['key2'].replace(mapping)\n",
    "\n",
    "\n",
    "USA['year'], USA['month'], USA['day'] = USA['TIME'].dt.year, USA['TIME'].dt.month,USA['TIME'].dt.day\n",
    "inflation['key2'] = inflation['year'].astype(str) + inflation['month'].astype(str) + inflation['day'].astype(str)\n",
    "USA['key'] = USA['year'].astype(str) + USA['month'].astype(str) + USA['day'].astype(str)\n",
    "mapping = USA.set_index('key').to_dict()['Value']\n",
    "\n",
    "inflation['Value'] = inflation['key2'].replace(mapping)\n",
    "\n",
    "USA['year'], USA['month'], USA['day'] = USA['TIME'].dt.year, USA['TIME'].dt.month,USA['TIME'].dt.day\n",
    "trade['key2'] = trade['year'].astype(str) + trade['month'].astype(str) + trade['day'].astype(str)\n",
    "USA['key'] = USA['year'].astype(str) + USA['month'].astype(str) + USA['day'].astype(str)\n",
    "mapping = USA.set_index('key').to_dict()['Value']\n",
    "\n",
    "trade['Value'] = trade['key2'].replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from OECD data, major changes in jan 2015 to sept 2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy['polarity_grouped'] = economy['polarity_grouped']*100\n",
    "economy['polarity_title_grouped'] = economy['polarity_title_grouped']*100\n",
    "\n",
    "inflation['polarity_grouped'] = inflation['polarity_grouped']*100\n",
    "inflation['polarity_title_grouped'] = inflation['polarity_title_grouped']*100\n",
    "\n",
    "wages['polarity_grouped'] = wages['polarity_grouped']*100\n",
    "wages['polarity_title_grouped'] = wages['polarity_title_grouped']*100\n",
    "\n",
    "unemployment['polarity_grouped'] = unemployment['polarity_grouped']*100\n",
    "unemployment['polarity_title_grouped'] = unemployment['polarity_title_grouped']*100\n",
    "\n",
    "rates['polarity_grouped'] = rates['polarity_grouped']*100\n",
    "rates['polarity_title_grouped'] = rates['polarity_title_grouped']*100\n",
    "\n",
    "trade['polarity_grouped'] = trade['polarity_grouped']*100\n",
    "trade['polarity_title_grouped'] = trade['polarity_title_grouped']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "\n",
    "# Plot the responses for different events and regions\n",
    "sns.lineplot(x=economy.index, y=\"polarity_title_grouped\",\n",
    "             data=economy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = economy[['polarity_grouped','polarity_title_grouped','Value']]\n",
    "model = VAR(data)\n",
    "results = model.fit(2)\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = rates[['polarity_grouped','polarity_title_grouped','Value']]\n",
    "model = VAR(data1)\n",
    "results1 = model.fit(2)\n",
    "results1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = unemployment[['polarity_grouped','polarity_title_grouped','Value']]\n",
    "model = VAR(data2)\n",
    "results2 = model.fit(2)\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = trade[['polarity_grouped','polarity_title_grouped','Value']]\n",
    "model = VAR(data3)\n",
    "results3 = model.fit(2)\n",
    "results3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = inflation[['polarity_grouped','polarity_title_grouped','Value']]\n",
    "model = VAR(data4)\n",
    "results4 = model.fit(2)\n",
    "results4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = wages[['polarity_grouped','polarity_title_grouped','Value']]\n",
    "model = VAR(data5)\n",
    "results5 = model.fit(2)\n",
    "results5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_order = results.k_ar\n",
    "\n",
    "results.forecast(data.values[-lag_order:], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_forecast(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irf = results.irf(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irf.plot(orth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the train and validation set\n",
    "train = data[:int(0.8*(len(data)))]\n",
    "valid = data[int(0.8*(len(data))):]\n",
    "\n",
    "#fit the model\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "model = VAR(endog=train)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# make prediction on validation\n",
    "#prediction = model_fit.forecast(model_fit.y, steps=len(valid))\n",
    "\n",
    "results5 = model.fit(2)\n",
    "results5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=12\n",
    "test = 'ssr_chi2test'\n",
    "df = inflation.copy()\n",
    "df = df[['polarity_title_grouped','Value']]\n",
    "  \n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "grangers_causation_matrix(df, variables = df.columns)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
